{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8577670,"sourceType":"datasetVersion","datasetId":5129479}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hkaragah/meta-stock-price?scriptVersionId=191834991\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Meta Stock Price Prediction\nThe objective of this notebook is to:\n* Load and preprocess the meta stock price data series\n* Train a deep model using DNN, CNN, and LSTM units using portion of the data\n* Utilize the trained model to predict the tail portion of the series (which has not used for training)","metadata":{}},{"cell_type":"markdown","source":"### 1. Import Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T18:19:28.092197Z","iopub.execute_input":"2024-08-09T18:19:28.092573Z","iopub.status.idle":"2024-08-09T18:19:42.856697Z","shell.execute_reply.started":"2024-08-09T18:19:28.092527Z","shell.execute_reply":"2024-08-09T18:19:42.8556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/meta-stock-price-dataset/Meta Dataset.csv'\n\ndf = pd.read_csv(file_path, index_col='Date')\ndisplay(df)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:49.779957Z","iopub.execute_input":"2024-08-09T18:20:49.780796Z","iopub.status.idle":"2024-08-09T18:20:49.833973Z","shell.execute_reply.started":"2024-08-09T18:20:49.780762Z","shell.execute_reply":"2024-08-09T18:20:49.832609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Explore Data","metadata":{}},{"cell_type":"code","source":"# Check the range of the values in each column\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:52.473203Z","iopub.execute_input":"2024-08-09T18:20:52.473574Z","iopub.status.idle":"2024-08-09T18:20:52.518951Z","shell.execute_reply.started":"2024-08-09T18:20:52.473545Z","shell.execute_reply":"2024-08-09T18:20:52.517634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns, except the Volume, are in the same order of magnitude. The volume column is in the order of 10^6 to 10^8","metadata":{}},{"cell_type":"code","source":"# Check the data type of each column and whether there is any null (NaN)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:54.742599Z","iopub.execute_input":"2024-08-09T18:20:54.74302Z","iopub.status.idle":"2024-08-09T18:20:54.765236Z","shell.execute_reply.started":"2024-08-09T18:20:54.742991Z","shell.execute_reply":"2024-08-09T18:20:54.763993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:57.233529Z","iopub.execute_input":"2024-08-09T18:20:57.233937Z","iopub.status.idle":"2024-08-09T18:20:57.243827Z","shell.execute_reply.started":"2024-08-09T18:20:57.233895Z","shell.execute_reply":"2024-08-09T18:20:57.242134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().any()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:57.99054Z","iopub.execute_input":"2024-08-09T18:20:57.990957Z","iopub.status.idle":"2024-08-09T18:20:58.000171Z","shell.execute_reply.started":"2024-08-09T18:20:57.990927Z","shell.execute_reply":"2024-08-09T18:20:57.999067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:20:58.568548Z","iopub.execute_input":"2024-08-09T18:20:58.568963Z","iopub.status.idle":"2024-08-09T18:20:58.577569Z","shell.execute_reply.started":"2024-08-09T18:20:58.56893Z","shell.execute_reply":"2024-08-09T18:20:58.576437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the data type of the index column\ndf.index.dtype","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:21:00.013724Z","iopub.execute_input":"2024-08-09T18:21:00.0147Z","iopub.status.idle":"2024-08-09T18:21:00.02113Z","shell.execute_reply.started":"2024-08-09T18:21:00.014663Z","shell.execute_reply":"2024-08-09T18:21:00.019941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data type 'O' is any Python object.","metadata":{}},{"cell_type":"code","source":"# Convert index to 'datetime' data type\ndf.index = pd.to_datetime(df.index)\ndf.index.dtype","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:21:01.997266Z","iopub.execute_input":"2024-08-09T18:21:01.997985Z","iopub.status.idle":"2024-08-09T18:21:02.011417Z","shell.execute_reply.started":"2024-08-09T18:21:01.997951Z","shell.execute_reply":"2024-08-09T18:21:02.010343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(10, 2 * len(df.columns)), sharex=True)\n\nfor col, ax in zip(df.columns, axes):\n    df[col].plot(ax=ax, title=col)\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Values')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:21:03.549706Z","iopub.execute_input":"2024-08-09T18:21:03.550115Z","iopub.status.idle":"2024-08-09T18:21:08.282752Z","shell.execute_reply.started":"2024-08-09T18:21:03.550086Z","shell.execute_reply":"2024-08-09T18:21:08.281239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All column values, except the Volume, demonstrate similar trends over time.","metadata":{}},{"cell_type":"markdown","source":"### 3. Create Dataset\nLet's use 'High' values to create a dataset.<br>\n*Disclaimer: most of the code in this section is copied over from course 4 in ref. [1].*","metadata":{}},{"cell_type":"code","source":"highs = np.array(df['High']) # numpy.ndarray of shape (3028,)\ntime = np.arange(len(highs)) # numpy.ndarray of shape (3028,)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:21:14.97231Z","iopub.execute_input":"2024-08-09T18:21:14.972721Z","iopub.status.idle":"2024-08-09T18:21:14.978293Z","shell.execute_reply.started":"2024-08-09T18:21:14.972691Z","shell.execute_reply":"2024-08-09T18:21:14.977071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'First 5 values of series: {highs[:5]}')\nprint(f'Last 5 values of series: {highs[-5:]}')\nprint(f'Length of the time series: {len(highs)}')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:04:41.305095Z","iopub.execute_input":"2024-06-29T02:04:41.305441Z","iopub.status.idle":"2024-06-29T02:04:41.310931Z","shell.execute_reply.started":"2024-06-29T02:04:41.305412Z","shell.execute_reply":"2024-06-29T02:04:41.31006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1. Split the Dataset","metadata":{}},{"cell_type":"code","source":"split_time = 2500\n\ntime_train = time[:split_time] # numpy.ndarray of shape (2500,)\nX_train = highs[:split_time] # numpy.ndarray of shape (2500,)\n\ntime_valid = time[split_time:] # numpy.ndarray of shape (2500,)\nX_valid = highs[split_time:] # numpy.ndarray of shape (2500,)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:22:37.33364Z","iopub.execute_input":"2024-08-09T18:22:37.334035Z","iopub.status.idle":"2024-08-09T18:22:37.33949Z","shell.execute_reply.started":"2024-08-09T18:22:37.334006Z","shell.execute_reply":"2024-08-09T18:22:37.338242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2. Prepare Features and Labels","metadata":{}},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"Generates dataset windows\n\n    Args:\n      series (array of float) - contains the values of the time series\n      window_size (int) - the number of time steps to include in the feature\n      batch_size (int) - the batch size\n      shuffle_buffer(int) - buffer size to use for the shuffle method\n\n    Returns:\n      dataset (TF Dataset) - TF Dataset containing time windows\n    \"\"\"\n  \n    explore = {} # Store converted data in each for further exploration\n    \n    # Generate a TF Dataset from the series values\n    dataset = tf.data.Dataset.from_tensor_slices(series) # Each value in series will be a Tensor\n    explore['from_tensor_slices'] = dataset\n          \n    # Window the data but only take those with the specified size\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    explore['window'] = dataset\n    \n    # Flatten the windows by putting its elements in a single batch\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    explore['flat_map'] = dataset\n\n    # Create tuples with features and labels \n    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n    explore['map'] = dataset\n\n    # Shuffle the windows\n    dataset = dataset.shuffle(shuffle_buffer)\n    explore['shuffle'] = dataset\n\n    # Create batches of windows\n    dataset = dataset.batch(batch_size).prefetch(1)\n    explore['batch'] = dataset\n\n    return dataset, explore","metadata":{"execution":{"iopub.status.busy":"2024-08-09T19:11:17.116438Z","iopub.execute_input":"2024-08-09T19:11:17.117182Z","iopub.status.idle":"2024-08-09T19:11:17.127426Z","shell.execute_reply.started":"2024-08-09T19:11:17.117148Z","shell.execute_reply":"2024-08-09T19:11:17.126234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 120\nbatch_size = 10\nshuffle_buffer_size = 1000\n\ntrain_set, explore = windowed_dataset(X_train, window_size, batch_size, shuffle_buffer_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T19:11:19.388016Z","iopub.execute_input":"2024-08-09T19:11:19.388393Z","iopub.status.idle":"2024-08-09T19:11:19.426962Z","shell.execute_reply.started":"2024-08-09T19:11:19.388367Z","shell.execute_reply":"2024-08-09T19:11:19.426104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's break down the steps and see how \"windowed_dataset\" method transforms \"X_train\" to \"train_set\".","metadata":{}},{"cell_type":"code","source":"# Explore slices\nslices = list(explore['from_tensor_slices'].as_numpy_iterator())\nprint(f\"Total no. of slices: {len(slices)}\\n\")\n\nprint(\"First 5 slices:\")\nfor count, element in enumerate(explore['from_tensor_slices'].take(5), start=1):\n    print(f\"Slice #{count}: {element.numpy()} of type {type(element)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T19:36:31.930242Z","iopub.execute_input":"2024-08-09T19:36:31.93122Z","iopub.status.idle":"2024-08-09T19:36:32.298549Z","shell.execute_reply.started":"2024-08-09T19:36:31.931183Z","shell.execute_reply":"2024-08-09T19:36:32.297449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"X_train is an numpy.ndarray of shape (2500,). The `tf.data.Dataset.from_tensor_slices(series)` transforms it to 2500 individual slice of type _tf.Tensor_ (`tensorflow.python.framework.ops.EagerTensor`).","metadata":{}},{"cell_type":"code","source":"# Explore windows\nfor count, window in enumerate(explore['window'], start=1):\n    if count in [1,2,3,4,5]:\n        print(f\"First 5 elements of window #{count}: {list(window.as_numpy_iterator())[:5]}\")\n        if count==1: window_len = len(list(window.as_numpy_iterator()))\n        \nprint(f\"\\nTotal no. of windows: {count}\")\nprint(f\"Window length: {window_len}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:33:53.640176Z","iopub.execute_input":"2024-08-09T20:33:53.640575Z","iopub.status.idle":"2024-08-09T20:33:54.45797Z","shell.execute_reply.started":"2024-08-09T20:33:53.640544Z","shell.execute_reply":"2024-08-09T20:33:54.45686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first five elements of the first 5 windows are printed above. Node that we used `shift=1` and by default `stride=1`. This measn to create a new set, the window is shifted one element. Furthermore, the `stride=1` determines the stride between input elements within a windows. See how the first element in each window is the second element of the previous window. This is because we set`shift=1`. Also, the total number of windows assuming `shift=1` is the total of slices minus the window size (2500-120=2380).<br>\nNote that the length of each window is 121. This is because we used `window_size + 1` to include the target in each window. Keep that in mind that the goal is use 120 values to predict the next one (i.e., 121st element's value).","metadata":{}},{"cell_type":"code","source":"#Explore flat_map\nfor count, batch in enumerate(explore['flat_map'].as_numpy_iterator(), start=1):\n    if count in [1,2,3]:\n        print(f\"Batch #{count}: shape={batch.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:15:00.559749Z","iopub.execute_input":"2024-08-09T20:15:00.560809Z","iopub.status.idle":"2024-08-09T20:15:01.118381Z","shell.execute_reply.started":"2024-08-09T20:15:00.560768Z","shell.execute_reply":"2024-08-09T20:15:01.11746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some highlights:\n\n* __.batch(...)__: The batch method in `lambda window: window.batch(window_size + 1)` groups consecutive elements int batches of specified size (window_size + 1).\n\n* __.flat_map(...)__: The flat_map method is useful to expand each element into multiple elements or datasets. It allows us to transform each element of the dataset into another dataset and then flatten the results into a single dataset.","metadata":{}},{"cell_type":"code","source":"# Explore map\nfor count, batch in enumerate(explore['map'].as_numpy_iterator(), start=1):\n    if count in [1,2,3,121,122,123]:\n        if count==121: print(\"...\")\n        print(f\"Batch #{count}: features[0:5]={batch[0][:5]}, target={batch[1]}\")\n            ","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:24:58.534776Z","iopub.execute_input":"2024-08-09T20:24:58.53519Z","iopub.status.idle":"2024-08-09T20:24:59.683722Z","shell.execute_reply.started":"2024-08-09T20:24:58.535158Z","shell.execute_reply":"2024-08-09T20:24:59.682848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `.map(...)` applies \"mapfunction\" to each element of the dataset and return a new dataset containing the transformed elements in the same order as they appeared in the input.<br>\nThe \"map function\" here is `lambda window: (window[:-1], window[-1])`. It transforms \"window\" with the shape of (121,) to a tuple((120,), (1,)) to generate the feature-target pair for each batch. Remember, out `window_size` is set to 120. When we created windows, we used `window_size + 1` to include the target to batch.","metadata":{}},{"cell_type":"markdown","source":"#### 3.3. Build the Model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n                      strides=1,\n                      activation=\"relu\",\n                      padding='causal',\n                      input_shape=[window_size, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n # Print the model summary \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T18:29:29.179773Z","iopub.execute_input":"2024-08-09T18:29:29.18054Z","iopub.status.idle":"2024-08-09T18:29:29.864807Z","shell.execute_reply.started":"2024-08-09T18:29:29.1805Z","shell.execute_reply":"2024-08-09T18:29:29.86378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4. Tune the Learning Rate","metadata":{}},{"cell_type":"code","source":"init_weights = model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:28:35.312868Z","iopub.execute_input":"2024-06-29T02:28:35.313485Z","iopub.status.idle":"2024-06-29T02:28:35.330308Z","shell.execute_reply.started":"2024-06-29T02:28:35.313457Z","shell.execute_reply":"2024-06-29T02:28:35.329607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the learning rate scheduler\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20))\n\n# Initialize the optimizer\noptimizer = tf.keras.optimizers.SGD(momentum=0.9)\n\n# Set the training parameters\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n\n# Train the model\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:19:46.378918Z","iopub.execute_input":"2024-06-29T02:19:46.379256Z","iopub.status.idle":"2024-06-29T02:20:41.399265Z","shell.execute_reply.started":"2024-06-29T02:19:46.37923Z","shell.execute_reply":"2024-06-29T02:20:41.398482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the learning rate array\nlrs = 1e-8 * (10 ** (np.arange(100) / 20))\n\n# Set the figure size\nplt.figure(figsize=(10, 6))\n\n# Set the grid\nplt.grid(True)\n\n# Plot the loss in log scale\nplt.semilogx(lrs, history.history[\"loss\"])\n\n# Increase the tickmarks size\nplt.tick_params('both', length=10, width=1, which='both')\n\n# Set the plot boundaries\nplt.axis([1e-8, 1e-3, 0, 100])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:20:47.804598Z","iopub.execute_input":"2024-06-29T02:20:47.805281Z","iopub.status.idle":"2024-06-29T02:20:48.401212Z","shell.execute_reply.started":"2024-06-29T02:20:47.80525Z","shell.execute_reply":"2024-06-29T02:20:48.400323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the results shown on the graph, I choose $2 \\times 10^{-7}$ for training the model.\n\n#### 3.5. Train the Model\nBefore starting the training with the best learning rate, we need to reset the trained weights to their pre-trained state.","metadata":{}},{"cell_type":"code","source":"# Reset states generated by Keras\ntf.keras.backend.clear_session()\n\n# Reset the weights\nmodel.set_weights(init_weights)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:28:43.354465Z","iopub.execute_input":"2024-06-29T02:28:43.355189Z","iopub.status.idle":"2024-06-29T02:28:43.624868Z","shell.execute_reply.started":"2024-06-29T02:28:43.355161Z","shell.execute_reply":"2024-06-29T02:28:43.623903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the learning rate\nlearning_rate = 2e-7\n\n# Set the optimizer \noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n\n# Set the training parameters\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:28:44.455756Z","iopub.execute_input":"2024-06-29T02:28:44.456591Z","iopub.status.idle":"2024-06-29T02:28:44.473552Z","shell.execute_reply.started":"2024-06-29T02:28:44.456552Z","shell.execute_reply":"2024-06-29T02:28:44.472635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_set,epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:28:45.52343Z","iopub.execute_input":"2024-06-29T02:28:45.523812Z","iopub.status.idle":"2024-06-29T02:33:38.12369Z","shell.execute_reply.started":"2024-06-29T02:28:45.523781Z","shell.execute_reply":"2024-06-29T02:33:38.122778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_series(x, y, format=\"-\", start=0, end=None, \n                title=None, xlabel=None, ylabel=None, legend=None ):\n    \"\"\"\n    Visualizes time series data\n\n    Args:\n      x (array of int) - contains values for the x-axis\n      y (array of int or tuple of arrays) - contains the values for the y-axis\n      format (string) - line style when plotting the graph\n      start (int) - first time step to plot\n      end (int) - last time step to plot\n      title (string) - title of the plot\n      xlabel (string) - label for the x-axis\n      ylabel (string) - label for the y-axis\n      legend (list of strings) - legend for the plot\n    \"\"\"\n\n    # Setup dimensions of the graph figure\n    plt.figure(figsize=(10, 6))\n    \n    # Check if there are more than two series to plot\n    if type(y) is tuple:\n\n      # Loop over the y elements\n      for y_curr in y:\n\n        # Plot the x and current y values\n        plt.plot(x[start:end], y_curr[start:end], format)\n\n    else:\n      # Plot the x and y values\n      plt.plot(x[start:end], y[start:end], format)\n\n    # Label the x-axis\n    plt.xlabel(xlabel)\n\n    # Label the y-axis\n    plt.ylabel(ylabel)\n\n    # Set the legend\n    if legend:\n      plt.legend(legend)\n\n    # Set the title\n    plt.title(title)\n\n    # Overlay a grid on the graph\n    plt.grid(True)\n\n    # Draw the graph on screen\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:35:59.551984Z","iopub.execute_input":"2024-06-29T02:35:59.553015Z","iopub.status.idle":"2024-06-29T02:35:59.56223Z","shell.execute_reply.started":"2024-06-29T02:35:59.55298Z","shell.execute_reply":"2024-06-29T02:35:59.561367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get mae and loss from history log\nmae=history.history['mae']\nloss=history.history['loss']\n\n# Get number of epochs\nepochs=range(len(loss)) \n\n# Plot mae and loss\nplot_series(\n    x=epochs, \n    y=(mae, loss), \n    title='MAE and Loss', \n    xlabel='MAE',\n    ylabel='Loss',\n    legend=['MAE', 'Loss']\n    )\n\n# Only plot the last 80% of the epochs\nzoom_split = int(epochs[-1] * 0.2)\nepochs_zoom = epochs[zoom_split:]\nmae_zoom = mae[zoom_split:]\nloss_zoom = loss[zoom_split:]\n\n# Plot zoomed mae and loss\nplot_series(\n    x=epochs_zoom, \n    y=(mae_zoom, loss_zoom), \n    title='MAE and Loss', \n    xlabel='MAE',\n    ylabel='Loss',\n    legend=['MAE', 'Loss']\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:36:01.065078Z","iopub.execute_input":"2024-06-29T02:36:01.065426Z","iopub.status.idle":"2024-06-29T02:36:01.623949Z","shell.execute_reply.started":"2024-06-29T02:36:01.0654Z","shell.execute_reply":"2024-06-29T02:36:01.623035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.6. Model Prediction","metadata":{}},{"cell_type":"code","source":"def model_forecast(model, series, window_size, batch_size):\n    \"\"\"Uses an input model to generate predictions on data windows\n\n    Args:\n      model (TF Keras Model) - model that accepts data windows\n      series (array of float) - contains the values of the time series\n      window_size (int) - the number of time steps to include in the window\n      batch_size (int) - the batch size\n\n    Returns:\n      forecast (numpy array) - array containing predictions\n    \"\"\"\n\n    # Generate a TF Dataset from the series values\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n\n    # Window the data but only take those with the specified size\n    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n\n    # Flatten the windows by putting its elements in a single batch\n    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n    \n    # Create batches of windows\n    dataset = dataset.batch(batch_size).prefetch(1)\n    \n    # Get predictions on the entire dataset\n    forecast = model.predict(dataset)\n    \n    return forecast","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:36:05.447646Z","iopub.execute_input":"2024-06-29T02:36:05.44824Z","iopub.status.idle":"2024-06-29T02:36:05.45467Z","shell.execute_reply.started":"2024-06-29T02:36:05.448207Z","shell.execute_reply":"2024-06-29T02:36:05.453755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce the original series\nforecast_series = highs[split_time-window_size:-1]\n\n# Use helper function to generate predictions\nforecast = model_forecast(model, forecast_series, window_size, batch_size)\n\n# Drop single dimensional axis\nresults = forecast.squeeze()\n\n# Plot the results\nplot_series(time_valid, (X_valid, results))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T02:36:06.585456Z","iopub.execute_input":"2024-06-29T02:36:06.585814Z","iopub.status.idle":"2024-06-29T02:36:07.517647Z","shell.execute_reply.started":"2024-06-29T02:36:06.585784Z","shell.execute_reply":"2024-06-29T02:36:07.516648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The prediction (after the timestamp of 2500) is obviousely off, and the model couldn't predict the sudden rise the in the high value. This could be due the fact that the model never saw such a sharp rise in the training set.<br>\nNonetheless, the model successfully predicted the dip that happened shortly after 3000 timestamp.","metadata":{}},{"cell_type":"markdown","source":"### Reference\n[ 1. Tensorflow Developer Professional Certificate by DeepLearning.AI](https://www.coursera.org/professional-certificates/tensorflow-in-practice)","metadata":{}}]}